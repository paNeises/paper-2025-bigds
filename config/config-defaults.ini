# PLEASE DO NOT JUST EDIT THIS FILE BY HAND! INSTRUCTIONS BELOW!
#
# This file contains the default configuration for the tool, which is loaded
# on every run. In order to configure the tool, please create a file called
# "config.ini" aside this file and put the configuration, that you want to
# change in this file. For example, to enable debugging and using the cpu, the
# file should contain:
# -------
# [general]
# debug = 1
# [gpt4all]
# device = cpu
# -------
# Following, you find all configuration options for the tool together with an
# explanation on what they do.


# -----------------------------------------------------------------------------
# This section contains the general tool configuration.
# -----------------------------------------------------------------------------
[general]

# Decides wether the tool should give debug output.
#
# Possible values: 0 (no), 1 (yes)
debug = 0


# -----------------------------------------------------------------------------
# This section configures how the example dataset (D3 subset) is created.
# -----------------------------------------------------------------------------
[example_dataset]

# The following section configures the settings for the run mode, which is used
# to create the example dataset.
# There are three possible modes in general:
# 1. Extract all publications from the D3 dataset.
# 2. Extract a limited random number of publications.
# 3. Extract a user-defined set of publications: The set must be specified by
#    an additionally supplied list (id_list_json) which contains the id of
#    each publication document.
#
# Possible values: all, ids, random
creation_mode = ids

# The number of publications, that should be extracted from the D3 dataset by
# the keywordextractor if the creation_mode is set to random.
#
# Possible values: An integer > 0
num_random_publications = 2500

# The path of the file containing the id list for the creation_mode ids. This
# file should be a .json file containing a list of ids.
#
# Possible values: An absolute path to the file (or a path relative to the tool
# directory)
id_list_json = reproducibility/dataset_D3_ID_list.json


# -----------------------------------------------------------------------------
# This section configures the dataset that should be used.
# -----------------------------------------------------------------------------
[dataset]

# The path to the file contain the metadata for the tool to annotate. The
# tool expects a .json file containing a list of elements to annotate, which
# are represented by maps. For an example on how this should look, please refer
# to the example dataset, which can be downloaded by the tool with the
# "download-example-dataset" subcommand and is configured here as default.
#
# Possible values: An absolute path to the file (or a path relative to the tool
# directory)
metadata_json = data/assets_example/metadata.json

# The path to the file containing the target values for the annotation. The
# tool expects a .json file contain a list of strings. For an example on how
# this should look, please also refer to the example dataset, which can be
# downloaded by the tool with the "download-example-dataset" subcommand and is
# configured here as default value.
#
# Possible values: An absolute path to the file (or a path relative to the tool
# directory)
targets_json = data/assets_example/targets.json

# The path to the file, in which the result of the annotation process should be
# written. This file must not exist in order to run the "annotate" subcommand,
# because this command creates it and aborts if it exists (by that, the tool
# prevents to overwrite any valuable output of earlier runs). After that, it
# must exist and is used by the "evaluate" subcommand, which evaluates a
# computed annotation.
#
# Possible values: An absolute path to the file (or a path relative to the tool
# directory)
annotated_json = data/assets_example/annotated_metadata.json

# The field name of the document data in the map that represents the elements
# to annotate from the metadata file.
#
# Possible values: A map field name.
document_index = title

# The field name that should be added in the map that represents the elements
# to annotate from the metadata file, which will contain the annotation result.
#
# Possible values: A map field name.
annotation_index = keywordextractor_annotation

# Indicates, wether the given metadata contains evaluation data. An evaluation
# can only be done, when this value is set to 1. The tool expects this data to
# be a list of targets stored in each map representing an element, that should
# be annotated. The field name for this data is configured below.
#
# Possible values = 0, 1
has_evaluation_data = 1

# The field name of the evaluation data in the map that represents the elements
# to annotate from the metadata file.
#
# Possible values: A map field name.
evaluation_data_index = subjects


# -----------------------------------------------------------------------------
# This section configures the LLM, that is used for the annotation process.
# -----------------------------------------------------------------------------
[gpt4all]

# The device that should be used to run the LLM.
#
# Possible values: cpu, gpu
device = gpu

# The LLM that is used for the annotation process. See
# https://docs.gpt4all.io/gpt4all_python/home.html for more information.
#
# Possible values: Meta-Llama-3-8B-Instruct.Q4_0.gguf,
# Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf, Phi-3-mini-4k-instruct.Q4_0.gguf,
# orca-mini-3b-gguf2-q4_0.gguf, gpt4all-13b-snoozy-q4_0.gguf
model_name = Meta-Llama-3-8B-Instruct.Q4_0.gguf


# -----------------------------------------------------------------------------
# This section configures the prompts, that are sent to the LLM in order to
# annotate the documents.
# -----------------------------------------------------------------------------
[prompt]

# The name, which is used to refer to the documents in the prompt. In order to
# build the plural form, an -s is appended.
#
# Possible values: A sting.
document_name = title

# The name, which is used to refer to the targets in the prompt. In order to
# build the plural form, an -s is appended.
#
# Possible values: A sting.
target_name = topic

# The number of targets, that should be obtained for the documents in the
# annotation process.
#
# Possible values: An integer > 0.
num_targets = 1

# The number of targets, that should be presented to the LLM at once in order
# to be added to the list of possible targets. The process of presenting the
# target list to the LLM is to first create an empty list of targets and then
# fill it with multiple prompts, each containing this number of targets, that
# should be added at once. This process is used in order to not create prompts
# that exceed the prompt length, the model is trained for.
#
# Possible values: An integer > 0.
targets_per_prompt = 25

# The number of prompts containing targets, that are used per chat session.
# The tool then creates multiple chat sessions, to present all targets to the
# LLM and get results for every part of the target list. In the end the tool
# combines the output of all chat sessions and lets the LLM pick the best
# targets of all chat sessions.
#
# Possible values: An integer > 0
prompts_per_chat = 5
